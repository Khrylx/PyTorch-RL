{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import os\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from larocs_sim.envs.drone_env import DroneEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.getcwd()+'/..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--env-name G] [--model-path G] [--render]\n",
      "                             [--seed N] [--max-expert-state-num N] [--H N]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/gabriel/.local/share/jupyter/runtime/kernel-b6d631eb-7a95-4478-bb1f-5d6cb366437a.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/anaconda3/envs/pyrep_raw/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3327: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Save expert trajectory')\n",
    "parser.add_argument('--env-name', default=\"Hopper-v2\", metavar='G',\n",
    "                    help='name of the environment to run')\n",
    "parser.add_argument('--model-path', metavar='G',\n",
    "                    help='name of the expert model')\n",
    "parser.add_argument('--render', action='store_true', default=False,\n",
    "                    help='render the environment')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='N',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--max-expert-state-num', type=int, default=50000, metavar='N',\n",
    "                    help='maximal number of main iterations (default: 50000)')\n",
    "parser.add_argument('--H', type=int, default=250, metavar='N',\n",
    "                    help='Time horizon of each episode (default: 250)')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '../assets/learned_models/Teste_ppo_209.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "args = Args()\n",
    "args.env_name = 'Drone'\n",
    "args.file = fname\n",
    "args.env_reset_mode = \"Gaussian\"\n",
    "args.seed=42\n",
    "args.max_expert_state_num=1000\n",
    "args.H=250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scene =  /home/gabriel/repos/my_larocs_sim/larocs_sim/envs/../../scenes/ardrone_modeled_headless.ttt\n",
      "headless =  True\n",
      "initial_position =  [0.0, 0.0, 1.7000000476837158]\n",
      "initial_orientation =  [-0.0, 0.0, -0.0]\n",
      "Initial Position of the Target =  [0.0, 8.419156038996789e-09, 1.7000000476837158]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/anaconda3/envs/pyrep_raw/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float64\n",
    "torch.set_default_dtype(dtype)\n",
    "env = DroneEnv(random=args.env_reset_mode,seed=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "is_disc_action = len(env.action_space.shape) == 0\n",
    "state_dim = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net, _, running_state = pickle.load(open(args.file, \"rb\"))\n",
    "running_state.fix = True\n",
    "expert_traj = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_loop():\n",
    "\n",
    "    num_steps = 0\n",
    "\n",
    "    for i_episode in count():\n",
    "\n",
    "        state = env.reset()\n",
    "        state = running_state(state)\n",
    "        reward_episode = 0\n",
    "\n",
    "        for t in range(args.H):\n",
    "            if num_steps >= args.max_expert_state_num:\n",
    "                return expert_traj\n",
    "            \n",
    "            state_var = tensor(state).unsqueeze(0).to(dtype)\n",
    "            # choose mean action\n",
    "            action = policy_net(state_var)[0][0].detach().numpy()\n",
    "            # action = policy_net.select_action(state_var)[0].cpu().numpy()\n",
    "            action = int(action) if is_disc_action else action.astype(np.float64)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = running_state(next_state)\n",
    "            reward_episode += reward\n",
    "            num_steps += 1\n",
    "\n",
    "            expert_traj.append(np.hstack([state, action]))\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        print('Episode {}\\t reward: {:.2f}'.format(i_episode, reward_episode))\n",
    "\n",
    "        if num_steps >= args.max_expert_state_num:\n",
    "            break\n",
    "    return expert_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collect_samples(pid, queue, env, policy, custom_reward,\n",
    "#                     mean_action, render, running_state, min_batch_size):\n",
    "\n",
    "def collect_samples():\n",
    "    #torch.randn(pid)\n",
    "    num_steps = 0\n",
    "\n",
    "    expert_traj = []\n",
    "    for i_episode in count():\n",
    "\n",
    "        state = env.reset()\n",
    "        state = running_state(state)\n",
    "        reward_episode = 0\n",
    "\n",
    "        for t in range(args.H):\n",
    "            if num_steps >= args.max_expert_state_num:\n",
    "                return expert_traj\n",
    "            \n",
    "            state_var = tensor(state).unsqueeze(0).to(dtype)\n",
    "            with torch.no_grad():\n",
    "                action = policy_net(state_var)[0][0].numpy() ## Chose mean action\n",
    "\n",
    "            action = int(action) if is_disc_action else action.astype(np.float64)\n",
    "\n",
    "            next_state, reward, done, _ = env.step(np.clip(action*100,a_min=-100, a_max=100))\n",
    "            next_state = running_state(next_state)\n",
    "\n",
    "            expert_traj.append(np.hstack([state, action]))\n",
    "            \n",
    "            mask = 0 if done else 1\n",
    "            reward_episode += reward\n",
    "\n",
    "\n",
    "            if done:\n",
    "                \n",
    "                break\n",
    "        \n",
    "            state = next_state\n",
    "        \n",
    "        print(\"Episode Reward = {0:.2f}\".format(reward_episode))\n",
    "        num_steps += (t + 1)\n",
    "\n",
    "\n",
    "    return expert_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'episode_reward' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-978dc6803feb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexpert_traj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-73-0ae87ee2dc5f>\u001b[0m in \u001b[0;36mcollect_samples\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episode Reward = {0:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mnum_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'episode_reward' is not defined"
     ]
    }
   ],
   "source": [
    "expert_traj = collect_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_traj = np.stack(expert_traj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((expert_traj, running_state), open(os.path.join(assets_dir(), 'expert_traj/{}_expert_traj.p'.format(args.env_name)), 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('pyrep_raw': conda)",
   "language": "python",
   "name": "python37564bitpyreprawconda5bc433778dee457a96e41a43a28ab451"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
